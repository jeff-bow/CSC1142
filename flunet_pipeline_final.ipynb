{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3bdf48-8ce6-4131-a1ee-9f7a5aaf700e",
   "metadata": {},
   "source": [
    "# Cloud Assignment\n",
    "## Luke Winters, Jeff Bowers\n",
    "### Data Pipeline for Global Influenza Surveillance Data Enriched with Population Data Using Apache Spark\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "### Let's start by getting a Spark session going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f831e9bd-4260-4ad4-a636-2a42032cbbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b4a216cb4314:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FluNetAssignment</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff7ce44290>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get a Spark session\n",
    "spark = SparkSession.builder.appName(\"FluNetAssignment\").getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93cf1e3-3220-4c5d-be64-1fb026e04732",
   "metadata": {},
   "source": [
    "## Bronze Layer - Raw FluNet Ingestion and Initial Cleaning\n",
    "### We can union the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa238c0-4cb6-4210-a9c6-c115b975ab21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows across all WHO regions: 184351\n",
      "root\n",
      " |-- WHOREGION: string (nullable = true)\n",
      " |-- FLUSEASON: string (nullable = true)\n",
      " |-- HEMISPHERE: string (nullable = true)\n",
      " |-- ITZ: string (nullable = true)\n",
      " |-- COUNTRY_CODE: string (nullable = true)\n",
      " |-- COUNTRY_AREA_TERRITORY: string (nullable = true)\n",
      " |-- ISO_WEEKSTARTDATE: string (nullable = true)\n",
      " |-- ISO_YEAR: string (nullable = true)\n",
      " |-- ISO_WEEK: string (nullable = true)\n",
      " |-- MMWR_WEEKSTARTDATE: string (nullable = true)\n",
      " |-- MMWR_YEAR: integer (nullable = true)\n",
      " |-- MMWR_WEEK: string (nullable = true)\n",
      " |-- ORIGIN_SOURCE: string (nullable = true)\n",
      " |-- SPEC_PROCESSED_NB: integer (nullable = true)\n",
      " |-- SPEC_RECEIVED_NB: integer (nullable = true)\n",
      " |-- AH1N12009: integer (nullable = true)\n",
      " |-- AH1: integer (nullable = true)\n",
      " |-- AH3: integer (nullable = true)\n",
      " |-- AH5: integer (nullable = true)\n",
      " |-- AH7N9: string (nullable = true)\n",
      " |-- ANOTSUBTYPED: integer (nullable = true)\n",
      " |-- ANOTSUBTYPABLE: string (nullable = true)\n",
      " |-- AOTHER_SUBTYPE: integer (nullable = true)\n",
      " |-- AOTHER_SUBTYPE_DETAILS: string (nullable = true)\n",
      " |-- INF_A: string (nullable = true)\n",
      " |-- BVIC_2DEL: string (nullable = true)\n",
      " |-- BVIC_3DEL: string (nullable = true)\n",
      " |-- BVIC_NODEL: integer (nullable = true)\n",
      " |-- BVIC_DELUNK: string (nullable = true)\n",
      " |-- BYAM: integer (nullable = true)\n",
      " |-- BNOTDETERMINED: integer (nullable = true)\n",
      " |-- INF_B: integer (nullable = true)\n",
      " |-- INF_ALL: integer (nullable = true)\n",
      " |-- INF_NEGATIVE: integer (nullable = true)\n",
      " |-- ILI_ACTIVITY: integer (nullable = true)\n",
      " |-- ADENO: integer (nullable = true)\n",
      " |-- BOCA: integer (nullable = true)\n",
      " |-- HUMAN_CORONA: string (nullable = true)\n",
      " |-- METAPNEUMO: integer (nullable = true)\n",
      " |-- PARAINFLUENZA: integer (nullable = true)\n",
      " |-- RHINO: integer (nullable = true)\n",
      " |-- RSV_PROCESSED: integer (nullable = true)\n",
      " |-- RSV: integer (nullable = true)\n",
      " |-- OTHERRESPVIRUS: integer (nullable = true)\n",
      " |-- OTHER_RESPVIRUS_DETAILS: string (nullable = true)\n",
      " |-- LAB_RESULT_COMMENT: string (nullable = true)\n",
      " |-- WCR_COMMENT: string (nullable = true)\n",
      " |-- ISO2: string (nullable = true)\n",
      " |-- ISOYW: string (nullable = true)\n",
      " |-- MMWRYW: string (nullable = true)\n",
      " |-- PSOURCE_SUBTYPE_INF: integer (nullable = true)\n",
      " |-- PSOURCE_PPOS_INF: integer (nullable = true)\n",
      " |-- PSOURCE_RSV: integer (nullable = true)\n",
      "\n",
      "+---------+---------+----------+-----------+------------+----------------------+-----------------+--------+--------+------------------+---------+---------+-------------+-----------------+----------------+---------+---+---+---+-----+------------+--------------+--------------+----------------------+-----+---------+---------+----------+-----------+----+--------------+-----+-------+------------+------------+-----+----+------------+----------+-------------+-----+-------------+----+--------------+-----------------------+------------------+-----------+----+------+------+-------------------+----------------+-----------+\n",
      "|WHOREGION|FLUSEASON|HEMISPHERE|        ITZ|COUNTRY_CODE|COUNTRY_AREA_TERRITORY|ISO_WEEKSTARTDATE|ISO_YEAR|ISO_WEEK|MMWR_WEEKSTARTDATE|MMWR_YEAR|MMWR_WEEK|ORIGIN_SOURCE|SPEC_PROCESSED_NB|SPEC_RECEIVED_NB|AH1N12009|AH1|AH3|AH5|AH7N9|ANOTSUBTYPED|ANOTSUBTYPABLE|AOTHER_SUBTYPE|AOTHER_SUBTYPE_DETAILS|INF_A|BVIC_2DEL|BVIC_3DEL|BVIC_NODEL|BVIC_DELUNK|BYAM|BNOTDETERMINED|INF_B|INF_ALL|INF_NEGATIVE|ILI_ACTIVITY|ADENO|BOCA|HUMAN_CORONA|METAPNEUMO|PARAINFLUENZA|RHINO|RSV_PROCESSED| RSV|OTHERRESPVIRUS|OTHER_RESPVIRUS_DETAILS|LAB_RESULT_COMMENT|WCR_COMMENT|ISO2| ISOYW|MMWRYW|PSOURCE_SUBTYPE_INF|PSOURCE_PPOS_INF|PSOURCE_RSV|\n",
      "+---------+---------+----------+-----------+------------+----------------------+-----------------+--------+--------+------------------+---------+---------+-------------+-----------------+----------------+---------+---+---+---+-----+------------+--------------+--------------+----------------------+-----+---------+---------+----------+-----------+----+--------------+-----+-------+------------+------------+-----+----+------------+----------+-------------+-----+-------------+----+--------------+-----------------------+------------------+-----------+----+------+------+-------------------+----------------+-----------+\n",
      "|      AFR|       YR|        SH|FLU_MID_AFR|         AGO|                Angola|         11-01-10|    2010|       2|          10-01-10|     2010|        2|   NOTDEFINED|               19|              19|        0|  0|  0|  0| NULL|           0|          NULL|             0|                  NULL|    0|     NULL|     NULL|         0|       NULL|   0|             0|    0|   NULL|        NULL|           1| NULL|NULL|        NULL|      NULL|         NULL| NULL|         NULL|NULL|          NULL|                   NULL|              NULL|       NULL|  AO|201002|201002|               NULL|            NULL|       NULL|\n",
      "|      AFR|       YR|        SH|FLU_MID_AFR|         AGO|                Angola|         25-01-10|    2010|       4|          24-01-10|     2010|        4|   NOTDEFINED|                0|               0|        0|  0|  0|  0| NULL|           0|          NULL|             0|                  NULL|    0|     NULL|     NULL|         0|       NULL|   0|             0|    0|   NULL|        NULL|           1| NULL|NULL|        NULL|      NULL|         NULL| NULL|         NULL|NULL|          NULL|                   NULL|              NULL|       NULL|  AO|201004|201004|               NULL|            NULL|       NULL|\n",
      "|      AFR|       YR|        SH|FLU_MID_AFR|         AGO|                Angola|         15-02-10|    2010|       7|          14-02-10|     2010|        7|   NOTDEFINED|                0|              25|        0|  0|  0|  0| NULL|           0|          NULL|             0|                  NULL|    0|     NULL|     NULL|         0|       NULL|   0|             0|    0|   NULL|        NULL|           1| NULL|NULL|        NULL|      NULL|         NULL| NULL|         NULL|NULL|          NULL|                   NULL|              NULL|       NULL|  AO|201007|201007|               NULL|            NULL|       NULL|\n",
      "|      AFR|       YR|        SH|FLU_MID_AFR|         AGO|                Angola|         01-03-10|    2010|       9|          28-02-10|     2010|        9|   NOTDEFINED|               30|              30|        0|  0|  0|  0| NULL|           0|          NULL|             0|                  NULL|    0|     NULL|     NULL|         0|       NULL|   0|             6|    6|      6|        NULL|           1| NULL|NULL|        NULL|      NULL|         NULL| NULL|         NULL|NULL|          NULL|                   NULL|              NULL|       NULL|  AO|201009|201009|               NULL|            NULL|       NULL|\n",
      "|      AFR|       YR|        SH|FLU_MID_AFR|         AGO|                Angola|         05-04-10|    2010|      14|          04-04-10|     2010|       14|   NOTDEFINED|                8|               8|        0|  0|  2|  0| NULL|           5|          NULL|             0|                  NULL|    7|     NULL|     NULL|         0|       NULL|   0|             0|    0|      7|        NULL|           1| NULL|NULL|        NULL|      NULL|         NULL| NULL|         NULL|NULL|          NULL|                   NULL|              NULL|       NULL|  AO|201014|201014|               NULL|            NULL|       NULL|\n",
      "+---------+---------+----------+-----------+------------+----------------------+-----------------+--------+--------+------------------+---------+---------+-------------+-----------------+----------------+---------+---+---+---+-----+------------+--------------+--------------+----------------------+-----+---------+---------+----------+-----------+----+--------------+-----+-------+------------+------------+-----+----+------------+----------+-------------+-----+-------------+----+--------------+-----------------------+------------------+-----------+----+------+------+-------------------+----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Read each WHO region file\n",
    "df_afr = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"data/FluNet_AFR.csv\")\n",
    ")\n",
    "\n",
    "df_amr = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"data/FluNet_AMR.csv\")\n",
    ")\n",
    "\n",
    "df_emr = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"data/FluNet_EMR.csv\")\n",
    ")\n",
    "\n",
    "df_eur = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"data/FluNet_EUR.csv\")\n",
    ")\n",
    "\n",
    "df_sear = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"data/FluNet_SEAR.csv\")\n",
    ")\n",
    "\n",
    "df_wpr = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"data/FluNet_WPR.csv\")\n",
    ")\n",
    "\n",
    "# Combine them into one big raw DataFrame\n",
    "df_raw = (\n",
    "    df_afr\n",
    "    .unionByName(df_amr)\n",
    "    .unionByName(df_emr)\n",
    "    .unionByName(df_eur)\n",
    "    .unionByName(df_sear)\n",
    "    .unionByName(df_wpr)\n",
    ")\n",
    "\n",
    "print(\"Total rows across all WHO regions:\", df_raw.count())\n",
    "\n",
    "# Quick look at schema and some rows\n",
    "df_raw.printSchema()\n",
    "df_raw.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bfb586-4886-44d5-9103-ae1c136b7e3a",
   "metadata": {},
   "source": [
    "### Now we're going to do some additional null profiling to ensure data quality. We'll do this by getting an idea of how many nulls are in each of the most important columns, by percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d29a19-3543-4a17-9ccf-3eefbb470176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of nulls in key columns (raw df_raw):\n",
      "+------------------+---------------------+-------------------------------+-----------------+-----------------+--------------------------+----------------+---------------------+\n",
      "|WHOREGION_null_pct|COUNTRY_CODE_null_pct|COUNTRY_AREA_TERRITORY_null_pct|ISO_YEAR_null_pct|ISO_WEEK_null_pct|SPEC_PROCESSED_NB_null_pct|INF_ALL_null_pct|INF_NEGATIVE_null_pct|\n",
      "+------------------+---------------------+-------------------------------+-----------------+-----------------+--------------------------+----------------+---------------------+\n",
      "|0.0               |3.37                 |3.48                           |3.82             |3.83             |10.47                     |46.99           |62.71                |\n",
      "+------------------+---------------------+-------------------------------+-----------------+-----------------+--------------------------+----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "important_cols = [\n",
    "    \"WHOREGION\",\n",
    "    \"COUNTRY_CODE\",\n",
    "    \"COUNTRY_AREA_TERRITORY\",\n",
    "    \"ISO_YEAR\",\n",
    "    \"ISO_WEEK\",\n",
    "    \"SPEC_PROCESSED_NB\",\n",
    "    \"INF_ALL\",\n",
    "    \"INF_NEGATIVE\"\n",
    "]\n",
    "\n",
    "# Calculate percentage of NULL values for each important column in the raw dataset\n",
    "null_profile_raw = (\n",
    "    df_raw\n",
    "    .select([\n",
    "        F.round(\n",
    "            F.avg(F.col(c).isNull().cast(\"double\")) * 100.0, 2\n",
    "        ).alias(f\"{c}_null_pct\")\n",
    "        for c in important_cols\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(\"Percentage of nulls in key columns (raw df_raw):\")\n",
    "null_profile_raw.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7978202f-7b25-4ee4-89ad-a490776f164c",
   "metadata": {},
   "source": [
    "### We do a quick count to make sure the union included all lines from the files individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31fa04fa-22ac-4e6d-b25f-5fa082c1f549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFR rows:  22627\n",
      "AMR rows:  41427\n",
      "EMR rows:  13670\n",
      "EUR rows:  77974\n",
      "SEAR rows: 8508\n",
      "WPR rows:  20145\n",
      "\n",
      "Total rows from individual parts: 184351\n",
      "df_raw.count():                    184351\n"
     ]
    }
   ],
   "source": [
    "# Row counts per regional file\n",
    "count_afr  = df_afr.count()\n",
    "count_amr  = df_amr.count()\n",
    "count_emr  = df_emr.count()\n",
    "count_eur  = df_eur.count()\n",
    "count_sear = df_sear.count()\n",
    "count_wpr  = df_wpr.count()\n",
    "\n",
    "print(\"AFR rows: \", count_afr)\n",
    "print(\"AMR rows: \", count_amr)\n",
    "print(\"EMR rows: \", count_emr)\n",
    "print(\"EUR rows: \", count_eur)\n",
    "print(\"SEAR rows:\", count_sear)\n",
    "print(\"WPR rows: \", count_wpr)\n",
    "\n",
    "\n",
    "# Add all the counts together, to make sure it all equals\n",
    "total_from_parts = (\n",
    "    count_afr + count_amr + count_emr +\n",
    "    count_eur + count_sear + count_wpr\n",
    ")\n",
    "\n",
    "print(\"\\nTotal rows from individual parts:\", total_from_parts)\n",
    "print(\"df_raw.count():                   \", df_raw.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53305194-0123-4dc2-9a90-3759d19ac46a",
   "metadata": {},
   "source": [
    "### Check for duplicates: First we check for exact duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3699e41-c4ef-446e-a62b-cbbc59de7de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows:    184351\n",
      "Distinct rows: 183799\n",
      "Duplicate rows (exact matches): 552\n"
     ]
    }
   ],
   "source": [
    "# Exact duplicate check (whole row)\n",
    "total_rows = df_raw.count()\n",
    "distinct_rows = df_raw.distinct().count()\n",
    "\n",
    "print(\"Total rows:   \", total_rows)\n",
    "print(\"Distinct rows:\", distinct_rows)\n",
    "print(\"Duplicate rows (exact matches):\", total_rows - distinct_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652162e-2d7e-4dc0-bbc7-f840bf522ce1",
   "metadata": {},
   "source": [
    "### Check for duplicates: Next we check for duplicate keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8705c3cc-73de-4607-bb68-5c1ce05f98d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of key combinations with duplicates: 36635\n",
      "+--------------------+------------+--------+--------+-----+\n",
      "|           WHOREGION|COUNTRY_CODE|ISO_YEAR|ISO_WEEK|count|\n",
      "+--------------------+------------+--------+--------+-----+\n",
      "|                 AFR|         GIN|    2023|       2|    2|\n",
      "|                 AFR|         GIN|    2023|       4|    2|\n",
      "|                 AFR|         MDG|    2021|      44|    2|\n",
      "|                 AFR|         SLE|    2022|      23|    2|\n",
      "|                 AFR|         SLE|    2022|      24|    2|\n",
      "|                 AFR|         SLE|    2022|      22|    2|\n",
      "|                 AFR|         SLE|    2022|      15|    2|\n",
      "|                 AFR|         GIN|    2023|       6|    2|\n",
      "|Case 2. Female 4 ...|        NULL|    NULL|    NULL|    2|\n",
      "|                 AFR|         GIN|    2023|       7|    2|\n",
      "+--------------------+------------+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key_cols = [\"WHOREGION\", \"COUNTRY_CODE\", \"ISO_YEAR\", \"ISO_WEEK\"]\n",
    "\n",
    "dup_keys = (\n",
    "    df_raw\n",
    "    .groupBy(key_cols)\n",
    "    .count()\n",
    "    .filter(F.col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "print(\"Number of key combinations with duplicates:\", dup_keys.count())\n",
    "dup_keys.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c5ee2b-1d2d-43ad-889b-e0868a87d760",
   "metadata": {},
   "source": [
    "## Removing exact duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec2aad9-a4eb-49c1-b1c4-4bc735617c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropDuplicates:\n",
      "Total rows:    183799\n",
      "Distinct rows: 183799\n"
     ]
    }
   ],
   "source": [
    "# Remove exact (row-wise) duplicates\n",
    "df_raw_nodup = df_raw.dropDuplicates()\n",
    "\n",
    "print(\"After dropDuplicates:\")\n",
    "print(\"Total rows:   \", df_raw_nodup.count())\n",
    "print(\"Distinct rows:\", df_raw_nodup.distinct().count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10626464-653a-4275-97eb-2b7a1199c244",
   "metadata": {},
   "source": [
    "### We need to filter out obviously “invalid” rows (like that WHOREGION = Case 2. Female 4 ...) and keep only real country–weeks.\n",
    "\n",
    "### This next step will do that by starting from df_raw_nodup (with exact duplicates already removed), and keep only rows where WHOREGION is one of the 6 real WHO Regions, and where COUNTRY_CODE, ISO_YEAR, ISO_WEEK are not null.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e89fed5-be2b-483e-af9c-ebe75f4be8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after filtering invalid records: 177224\n",
      "Number of key combinations with duplicates (after filtering): 36216\n",
      "+---------+------------+--------+--------+-----+\n",
      "|WHOREGION|COUNTRY_CODE|ISO_YEAR|ISO_WEEK|count|\n",
      "+---------+------------+--------+--------+-----+\n",
      "|      AMR|         COL|    2021|      45|    2|\n",
      "|      AMR|         GTM|    2023|      39|    2|\n",
      "|      AMR|         BRA|    2023|      32|    2|\n",
      "|      AMR|         USA|    2024|      14|    2|\n",
      "|      AMR|         USA|    2020|      21|    2|\n",
      "|      EMR|         AFG|    2023|      24|    2|\n",
      "|      EMR|         EGY|    2023|      40|    2|\n",
      "|      EMR|         ARE|    2023|      23|    2|\n",
      "|      EUR|         AZE|    2024|      28|    2|\n",
      "|      EUR|         ARM|    2014|      40|    2|\n",
      "+---------+------------+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Keep only rows with valid WHO region + non-null country/year/week\n",
    "valid_regions = [\"AFR\", \"AMR\", \"EMR\", \"EUR\", \"SEAR\", \"WPR\"]\n",
    "\n",
    "df_valid = (\n",
    "    df_raw_nodup\n",
    "    .filter(\n",
    "        (F.col(\"WHOREGION\").isin(valid_regions)) &\n",
    "        F.col(\"COUNTRY_CODE\").isNotNull() &\n",
    "        F.col(\"ISO_YEAR\").isNotNull() &\n",
    "        F.col(\"ISO_WEEK\").isNotNull()\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Rows after filtering invalid records:\", df_valid.count())\n",
    "\n",
    "# Check remaining duplicates by key: region + country + iso year/week\n",
    "key_cols = [\"WHOREGION\", \"COUNTRY_CODE\", \"ISO_YEAR\", \"ISO_WEEK\"]\n",
    "\n",
    "dup_keys = (\n",
    "    df_valid\n",
    "    .groupBy(key_cols)\n",
    "    .count()\n",
    "    .filter(F.col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "print(\"Number of key combinations with duplicates (after filtering):\", dup_keys.count())\n",
    "dup_keys.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478d9040-a456-4633-90e6-bff6da4017f6",
   "metadata": {},
   "source": [
    "### Next we are going to ensure that the years and weeks make sense. The FluNet dataset has records going back to 1995, so we'll start from there and ensure make sense (Only 52 weeks in a year so there can't be anything like Week 102, for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a22b540a-eaee-4dff-b405-b8be1a6066a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after enforcing year/week ranges: 177224\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Adding sensible year/week ranges\n",
    "df_valid = df_valid.filter(\n",
    "    (F.col(\"ISO_YEAR\") >= 1995) &       \n",
    "    (F.col(\"ISO_WEEK\").between(1, 53))\n",
    ")\n",
    "\n",
    "print(\"Rows after enforcing year/week ranges:\", df_valid.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c17d339-d21c-4502-9718-35da979ae557",
   "metadata": {},
   "source": [
    "### The raw FluNet data contains multiple records per country–week.\n",
    "### This pipeline will aggregate them to a single country–week record by summing all numeric surveillance counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd8c5402-b430-4cd2-bb9a-d6ae2ec7c398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after aggregation: 140108\n",
      "Number of key combinations with duplicates after aggregation: 0\n",
      "+---------+------------+----------------------+----------+-----------+--------+--------+-----------------+----------------+---------+----+---+---+------------+--------------+----------+----+--------------+-----+-------+------------+------------+-----+----+----------+-------------+-----+-------------+----+--------------+-------------------+----------------+-----------+\n",
      "|WHOREGION|COUNTRY_CODE|COUNTRY_AREA_TERRITORY|HEMISPHERE|        ITZ|ISO_YEAR|ISO_WEEK|SPEC_PROCESSED_NB|SPEC_RECEIVED_NB|AH1N12009| AH1|AH3|AH5|ANOTSUBTYPED|AOTHER_SUBTYPE|BVIC_NODEL|BYAM|BNOTDETERMINED|INF_B|INF_ALL|INF_NEGATIVE|ILI_ACTIVITY|ADENO|BOCA|METAPNEUMO|PARAINFLUENZA|RHINO|RSV_PROCESSED| RSV|OTHERRESPVIRUS|PSOURCE_SUBTYPE_INF|PSOURCE_PPOS_INF|PSOURCE_RSV|\n",
      "+---------+------------+----------------------+----------+-----------+--------+--------+-----------------+----------------+---------+----+---+---+------------+--------------+----------+----+--------------+-----+-------+------------+------------+-----+----+----------+-------------+-----+-------------+----+--------------+-------------------+----------------+-----------+\n",
      "|      AFR|         GAB|                 Gabon|        SH|FLU_MID_AFR|    2024|      32|                0|               0|        0|NULL|  0|  0|           0|             0|         0|   0|             0|    0|   NULL|           0|        NULL| NULL|NULL|      NULL|         NULL| NULL|         NULL|NULL|             0|               NULL|            NULL|       NULL|\n",
      "|      AFR|         SLE|          Sierra Leone|        NH|FLU_WST_AFR|    2018|      23|                2|               2|        0|NULL|  0|  0|           0|             0|         0|   0|             0|    0|   NULL|           2|           2|    0|   0|         0|            0|    0|         NULL|   0|             0|               NULL|            NULL|       NULL|\n",
      "|      AFR|         GIN|                Guinea|        NH|FLU_WST_AFR|    2020|      48|               21|              21|        0|   0|  0|  0|           0|          NULL|         1|   0|             0|    1|      1|        NULL|        NULL| NULL|NULL|      NULL|         NULL| NULL|         NULL|NULL|          NULL|               NULL|            NULL|       NULL|\n",
      "|      AFR|         MLI|                  Mali|        NH|FLU_WST_AFR|    2020|      34|               17|              17|        0|NULL|  2|  0|           0|          NULL|         0|   0|          NULL|    0|      2|        NULL|        NULL| NULL|NULL|      NULL|         NULL| NULL|         NULL|NULL|          NULL|               NULL|            NULL|       NULL|\n",
      "|      AFR|         NER|                 Niger|        NH|FLU_WST_AFR|    2011|       6|               24|              24|        7|NULL|  0|  0|           0|             0|         0|   0|             0|    0|      7|          17|           0| NULL|NULL|      NULL|         NULL| NULL|         NULL|NULL|          NULL|               NULL|            NULL|       NULL|\n",
      "+---------+------------+----------------------+----------+-----------+--------+--------+-----------------+----------------+---------+----+---+---+------------+--------------+----------+----+--------------+-----+-------+------------+------------+-----+----+----------+-------------+-----+-------------+----+--------------+-------------------+----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the grouping (key) columns\n",
    "group_cols = [\n",
    "    \"WHOREGION\",\n",
    "    \"COUNTRY_CODE\",\n",
    "    \"COUNTRY_AREA_TERRITORY\",\n",
    "    \"HEMISPHERE\",\n",
    "    \"ITZ\",\n",
    "    \"ISO_YEAR\",\n",
    "    \"ISO_WEEK\",\n",
    "]\n",
    "\n",
    "# Numeric columns we want to sum\n",
    "numeric_cols = [\n",
    "    \"SPEC_PROCESSED_NB\",\n",
    "    \"SPEC_RECEIVED_NB\",\n",
    "    \"AH1N12009\",\n",
    "    \"AH1\",\n",
    "    \"AH3\",\n",
    "    \"AH5\",\n",
    "    \"ANOTSUBTYPED\",\n",
    "    \"AOTHER_SUBTYPE\",\n",
    "    \"BVIC_NODEL\",\n",
    "    \"BYAM\",\n",
    "    \"BNOTDETERMINED\",\n",
    "    \"INF_B\",\n",
    "    \"INF_ALL\",\n",
    "    \"INF_NEGATIVE\",\n",
    "    \"ILI_ACTIVITY\",\n",
    "    \"ADENO\",\n",
    "    \"BOCA\",\n",
    "    \"METAPNEUMO\",\n",
    "    \"PARAINFLUENZA\",\n",
    "    \"RHINO\",\n",
    "    \"RSV_PROCESSED\",\n",
    "    \"RSV\",\n",
    "    \"OTHERRESPVIRUS\",\n",
    "    \"PSOURCE_SUBTYPE_INF\",\n",
    "    \"PSOURCE_PPOS_INF\",\n",
    "    \"PSOURCE_RSV\",\n",
    "]\n",
    "\n",
    "# Build aggregation expressions: sum each numeric column\n",
    "agg_exprs = [F.sum(F.col(c)).alias(c) for c in numeric_cols]\n",
    "\n",
    "# Aggregate to one row per (region, country, year, week)\n",
    "df_agg = df_valid.groupBy(*group_cols).agg(*agg_exprs)\n",
    "\n",
    "print(\"Rows after aggregation:\", df_agg.count())\n",
    "\n",
    "# Check that we no longer have duplicate keys\n",
    "dup_keys_after = (\n",
    "    df_agg\n",
    "    .groupBy(\"WHOREGION\", \"COUNTRY_CODE\", \"ISO_YEAR\", \"ISO_WEEK\")\n",
    "    .count()\n",
    "    .filter(F.col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "print(\"Number of key combinations with duplicates after aggregation:\",\n",
    "      dup_keys_after.count())\n",
    "\n",
    "df_agg.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e62887-0c2a-4370-9d6d-09f9a5b205d5",
   "metadata": {},
   "source": [
    "## Silver Layer - Cleaned Weekly, Country Level Dataset\n",
    "\n",
    "### So now we have exactly one row per region, country, ISO year, ISO week\n",
    "\n",
    "### We now make this data clean by giving nice names to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604d0c6d-571b-419a-a82f-bfa40e46af18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- who_region: string (nullable = true)\n",
      " |-- hemisphere: string (nullable = true)\n",
      " |-- itz: string (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- iso_year: integer (nullable = true)\n",
      " |-- iso_week: integer (nullable = true)\n",
      " |-- spec_received_nb: long (nullable = true)\n",
      " |-- spec_processed_nb: long (nullable = true)\n",
      " |-- ah1n1_2009: long (nullable = true)\n",
      " |-- ah1: long (nullable = true)\n",
      " |-- ah3: long (nullable = true)\n",
      " |-- ah5: long (nullable = true)\n",
      " |-- a_not_subtyped: long (nullable = true)\n",
      " |-- a_other_subtype: long (nullable = true)\n",
      " |-- b_vic_nodel: long (nullable = true)\n",
      " |-- b_yam: long (nullable = true)\n",
      " |-- b_not_determined: long (nullable = true)\n",
      " |-- inf_b: long (nullable = true)\n",
      " |-- inf_all: long (nullable = true)\n",
      " |-- inf_negative: long (nullable = true)\n",
      " |-- ili_activity: long (nullable = true)\n",
      " |-- adeno: long (nullable = true)\n",
      " |-- boca: long (nullable = true)\n",
      " |-- metapneumo: long (nullable = true)\n",
      " |-- parainfluenza: long (nullable = true)\n",
      " |-- rhino: long (nullable = true)\n",
      " |-- rsv_processed: long (nullable = true)\n",
      " |-- rsv: long (nullable = true)\n",
      " |-- other_resp_virus: long (nullable = true)\n",
      " |-- psource_subtype_inf: long (nullable = true)\n",
      " |-- psource_ppos_inf: long (nullable = true)\n",
      " |-- psource_rsv: long (nullable = true)\n",
      "\n",
      "+----------+----------+-----------+------------+------------+--------+--------+----------------+-----------------+----------+----+---+---+--------------+---------------+-----------+-----+----------------+-----+-------+------------+------------+-----+----+----------+-------------+-----+-------------+----+----------------+-------------------+----------------+-----------+\n",
      "|who_region|hemisphere|        itz|country_name|country_code|iso_year|iso_week|spec_received_nb|spec_processed_nb|ah1n1_2009| ah1|ah3|ah5|a_not_subtyped|a_other_subtype|b_vic_nodel|b_yam|b_not_determined|inf_b|inf_all|inf_negative|ili_activity|adeno|boca|metapneumo|parainfluenza|rhino|rsv_processed| rsv|other_resp_virus|psource_subtype_inf|psource_ppos_inf|psource_rsv|\n",
      "+----------+----------+-----------+------------+------------+--------+--------+----------------+-----------------+----------+----+---+---+--------------+---------------+-----------+-----+----------------+-----+-------+------------+------------+-----+----+----------+-------------+-----+-------------+----+----------------+-------------------+----------------+-----------+\n",
      "|       AFR|        SH|FLU_MID_AFR|       Gabon|         GAB|    2024|      32|               0|                0|         0|NULL|  0|  0|             0|              0|          0|    0|               0|    0|   NULL|           0|        NULL| NULL|NULL|      NULL|         NULL| NULL|         NULL|NULL|               0|               NULL|            NULL|       NULL|\n",
      "|       AFR|        NH|FLU_WST_AFR|Sierra Leone|         SLE|    2018|      23|               2|                2|         0|NULL|  0|  0|             0|              0|          0|    0|               0|    0|   NULL|           2|           2|    0|   0|         0|            0|    0|         NULL|   0|               0|               NULL|            NULL|       NULL|\n",
      "|       AFR|        NH|FLU_WST_AFR|      Guinea|         GIN|    2020|      48|              21|               21|         0|   0|  0|  0|             0|           NULL|          1|    0|               0|    1|      1|        NULL|        NULL| NULL|NULL|      NULL|         NULL| NULL|         NULL|NULL|            NULL|               NULL|            NULL|       NULL|\n",
      "|       AFR|        NH|FLU_WST_AFR|        Mali|         MLI|    2020|      34|              17|               17|         0|NULL|  2|  0|             0|           NULL|          0|    0|            NULL|    0|      2|        NULL|        NULL| NULL|NULL|      NULL|         NULL| NULL|         NULL|NULL|            NULL|               NULL|            NULL|       NULL|\n",
      "|       AFR|        NH|FLU_WST_AFR|       Niger|         NER|    2011|       6|              24|               24|         7|NULL|  0|  0|             0|              0|          0|    0|               0|    0|      7|          17|           0| NULL|NULL|      NULL|         NULL| NULL|         NULL|NULL|            NULL|               NULL|            NULL|       NULL|\n",
      "+----------+----------+-----------+------------+------------+--------+--------+----------------+-----------------+----------+----+---+---+--------------+---------------+-----------+-----+----------------+-----+-------+------------+------------+-----+----+----------+-------------+-----+-------------+----+----------------+-------------------+----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleaned, renamed version of the aggregated table\n",
    "df_clean = (\n",
    "    df_agg\n",
    "    .select(\n",
    "        # Dimensions\n",
    "        F.col(\"WHOREGION\").alias(\"who_region\"),\n",
    "        F.col(\"HEMISPHERE\").alias(\"hemisphere\"),\n",
    "        F.col(\"ITZ\").alias(\"itz\"),\n",
    "        F.col(\"COUNTRY_AREA_TERRITORY\").alias(\"country_name\"),\n",
    "        F.col(\"COUNTRY_CODE\").alias(\"country_code\"),\n",
    "        F.col(\"ISO_YEAR\").cast(\"int\").alias(\"iso_year\"),\n",
    "        F.col(\"ISO_WEEK\").cast(\"int\").alias(\"iso_week\"),\n",
    "\n",
    "        # Lab counts\n",
    "        F.col(\"SPEC_RECEIVED_NB\").alias(\"spec_received_nb\"),\n",
    "        F.col(\"SPEC_PROCESSED_NB\").alias(\"spec_processed_nb\"),\n",
    "        F.col(\"AH1N12009\").alias(\"ah1n1_2009\"),\n",
    "        F.col(\"AH1\").alias(\"ah1\"),\n",
    "        F.col(\"AH3\").alias(\"ah3\"),\n",
    "        F.col(\"AH5\").alias(\"ah5\"),\n",
    "        F.col(\"ANOTSUBTYPED\").alias(\"a_not_subtyped\"),\n",
    "        F.col(\"AOTHER_SUBTYPE\").alias(\"a_other_subtype\"),\n",
    "        F.col(\"BVIC_NODEL\").alias(\"b_vic_nodel\"),\n",
    "        F.col(\"BYAM\").alias(\"b_yam\"),\n",
    "        F.col(\"BNOTDETERMINED\").alias(\"b_not_determined\"),\n",
    "        F.col(\"INF_B\").alias(\"inf_b\"),\n",
    "        F.col(\"INF_ALL\").alias(\"inf_all\"),\n",
    "        F.col(\"INF_NEGATIVE\").alias(\"inf_negative\"),\n",
    "        F.col(\"ILI_ACTIVITY\").alias(\"ili_activity\"),\n",
    "\n",
    "        # Other respiratory viruses\n",
    "        F.col(\"ADENO\").alias(\"adeno\"),\n",
    "        F.col(\"BOCA\").alias(\"boca\"),\n",
    "        F.col(\"METAPNEUMO\").alias(\"metapneumo\"),\n",
    "        F.col(\"PARAINFLUENZA\").alias(\"parainfluenza\"),\n",
    "        F.col(\"RHINO\").alias(\"rhino\"),\n",
    "        F.col(\"RSV_PROCESSED\").alias(\"rsv_processed\"),\n",
    "        F.col(\"RSV\").alias(\"rsv\"),\n",
    "        F.col(\"OTHERRESPVIRUS\").alias(\"other_resp_virus\"),\n",
    "\n",
    "        # Source / meta counts\n",
    "        F.col(\"PSOURCE_SUBTYPE_INF\").alias(\"psource_subtype_inf\"),\n",
    "        F.col(\"PSOURCE_PPOS_INF\").alias(\"psource_ppos_inf\"),\n",
    "        F.col(\"PSOURCE_RSV\").alias(\"psource_rsv\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_clean.printSchema()\n",
    "df_clean.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75219803-4653-4a2b-a850-c1c9227777cc",
   "metadata": {},
   "source": [
    "### Next step in cleaning our dataset is handly negative counts and completely empty weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f27d37a-edc1-4c84-bbc3-c40823ba815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in df_clean after cleaning: 134983\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "count_cols = [\n",
    "    \"spec_received_nb\",\n",
    "    \"spec_processed_nb\",\n",
    "    \"inf_all\",\n",
    "    \"inf_negative\",\n",
    "]\n",
    "\n",
    "# Clip any negative lab counts to 0 \n",
    "for c in count_cols:\n",
    "    df_clean = df_clean.withColumn(\n",
    "        c,\n",
    "        F.when(F.col(c) < 0, 0).otherwise(F.col(c))\n",
    "    )\n",
    "\n",
    "# Drop rows that have no lab information at all\n",
    "df_clean = df_clean.filter(\n",
    "    (F.col(\"spec_processed_nb\").isNotNull()) |\n",
    "    (F.col(\"inf_all\").isNotNull()) |\n",
    "    (F.col(\"inf_negative\").isNotNull())\n",
    ")\n",
    "\n",
    "print(\"Rows in df_clean after cleaning:\", df_clean.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee41647-f8b1-4cd6-aca2-b091896c5610",
   "metadata": {},
   "source": [
    "### Next we can add a new feature: positivity rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19dfdaf9-46e2-4787-88d9-6617afb90dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+--------+---------------+-------+------------+--------------------+\n",
      "|who_region|        country_name|iso_year|iso_week|total_specimens|inf_all|inf_negative|     positivity_rate|\n",
      "+----------+--------------------+--------+--------+---------------+-------+------------+--------------------+\n",
      "|       AFR|              Guinea|    2020|      48|             21|      1|        NULL|0.047619047619047616|\n",
      "|       AFR|                Mali|    2020|      34|             17|      2|        NULL| 0.11764705882352941|\n",
      "|       AFR|               Niger|    2011|       6|             24|      7|          17|  0.2916666666666667|\n",
      "|       AFR|              Zambia|    2021|      28|             64|      5|          59|            0.078125|\n",
      "|       AFR|        South Africa|    2016|      20|            237|     29|         208| 0.12236286919831224|\n",
      "|       AFR|        Burkina Faso|    2022|      34|              3|      2|           1|  0.6666666666666666|\n",
      "|       AMR|               Chile|    2016|      10|            380|      7|         361|0.018421052631578946|\n",
      "|       AMR|           Argentina|    2003|      12|              1|      1|        NULL|                 1.0|\n",
      "|       AMR|              Panama|    2017|      39|            101|      7|          20| 0.06930693069306931|\n",
      "|       AMR|United States of ...|    2017|      13|          35530|   7625|       27905|  0.2146073740500985|\n",
      "+----------+--------------------+--------+--------+---------------+-------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add some derived features on top of df_clean\n",
    "df_features = (\n",
    "    df_clean\n",
    "    # Convenience alias for clarity\n",
    "    .withColumn(\"total_specimens\", F.col(\"spec_processed_nb\"))\n",
    "    # Positivity rate: fraction of processed specimens that were positive\n",
    "    .withColumn(\n",
    "        \"positivity_rate\",\n",
    "        F.when(\n",
    "            (F.col(\"spec_processed_nb\") > 0) & F.col(\"inf_all\").isNotNull(),\n",
    "            F.col(\"inf_all\") / F.col(\"spec_processed_nb\")\n",
    "        ).otherwise(None)  # avoid divide-by-zero / missing\n",
    "    )\n",
    ")\n",
    "\n",
    "# Look at a few rows where positivity_rate is not null\n",
    "df_features.select(\n",
    "    \"who_region\",\n",
    "    \"country_name\",\n",
    "    \"iso_year\",\n",
    "    \"iso_week\",\n",
    "    \"total_specimens\",\n",
    "    \"inf_all\",\n",
    "    \"inf_negative\",\n",
    "    \"positivity_rate\"\n",
    ").where(F.col(\"positivity_rate\").isNotNull()).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1b595-e84b-49bd-8d24-6f287df3bf59",
   "metadata": {},
   "source": [
    "## Next step in the pipeline is to create a 4-week rolling features for each country (current week + previous 3), and calculate the sum of positive flu cases and the moving average positivity rate. \n",
    "\n",
    "## The pipeline then removes weeks where nothing happens in the lab, eg. no specimens are processed, and no positive or negative results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcfd337d-a3f3-42a4-909d-e1c2b253b4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in df_features after rolling features & filtering: 128025\n",
      "+----------+------------+--------+--------+-------------------+-------------------+-------+---------------+\n",
      "|who_region|country_name|iso_year|iso_week|    positivity_rate|    pos_rate_4wk_ma|inf_all|inf_all_4wk_sum|\n",
      "+----------+------------+--------+--------+-------------------+-------------------+-------+---------------+\n",
      "|       AMR|       Aruba|    2017|       1|              0.275|              0.275|     11|             11|\n",
      "|       AMR|       Aruba|    2017|       2|               NULL|              0.275|   NULL|             11|\n",
      "|       AMR|       Aruba|    2017|       3|0.21428571428571427|0.24464285714285716|      3|             14|\n",
      "|       AMR|       Aruba|    2017|       4|0.08333333333333333| 0.1908730158730159|      1|             15|\n",
      "|       AMR|       Aruba|    2017|       5|               NULL| 0.1488095238095238|   NULL|              4|\n",
      "|       AMR|       Aruba|    2017|       6|0.16666666666666666|0.15476190476190477|      1|              5|\n",
      "|       AMR|       Aruba|    2017|       7|                0.2|               0.15|      1|              3|\n",
      "|       AMR|       Aruba|    2017|       8|0.18181818181818182|0.18282828282828287|      2|              4|\n",
      "|       AMR|       Aruba|    2017|       9|0.15384615384615385| 0.1755827505827506|      2|              6|\n",
      "|       AMR|       Aruba|    2017|      10|               NULL|0.17855477855477855|   NULL|              5|\n",
      "+----------+------------+--------+--------+-------------------+-------------------+-------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Temporal features + filtering structurally empty weeks\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 4-week rolling window by country (current week + previous 3)\n",
    "w_country_4wk = (\n",
    "    Window\n",
    "    .partitionBy(\"country_code\")\n",
    "    .orderBy(\"iso_year\", \"iso_week\")\n",
    "    .rowsBetween(-3, 0)\n",
    ")\n",
    "\n",
    "df_features = (\n",
    "    df_features\n",
    "    .withColumn(\n",
    "        \"inf_all_4wk_sum\",\n",
    "        F.sum(\"inf_all\").over(w_country_4wk)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"pos_rate_4wk_ma\",\n",
    "        F.avg(\"positivity_rate\").over(w_country_4wk)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Drop weeks with no specimens processed, no positives, and no negatives\n",
    "df_features = df_features.filter(\n",
    "    ~(\n",
    "        (F.col(\"spec_processed_nb\").isNull() | (F.col(\"spec_processed_nb\") == 0))\n",
    "        & F.col(\"inf_all\").isNull()\n",
    "        & F.col(\"inf_negative\").isNull()\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Rows in df_features after rolling features & filtering:\", df_features.count())\n",
    "df_features.select(\n",
    "    \"who_region\", \"country_name\", \"iso_year\", \"iso_week\",\n",
    "    \"positivity_rate\", \"pos_rate_4wk_ma\", \"inf_all\", \"inf_all_4wk_sum\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3389f7-3b13-485c-89a0-0a0ee18fbee8",
   "metadata": {},
   "source": [
    "### Next step in the data pipeline is to save what what has been done so far as a proper “processed” dataset. The dataset will be written out as Parquet, partitioned by year + region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7365616b-b4e7-47d4-98e8-7b9d27ea74d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written processed data to: data/processed/flunet_features\n"
     ]
    }
   ],
   "source": [
    "# Path inside the container (and under your cloud_assignment/data folder on Windows)\n",
    "output_path = \"data/processed/flunet_features\"\n",
    "\n",
    "(\n",
    "    df_features\n",
    "    .write\n",
    "    .mode(\"overwrite\")                 # safe to re-run pipeline\n",
    "    .partitionBy(\"iso_year\", \"who_region\")  # good for time/region queries\n",
    "    .parquet(output_path)\n",
    ")\n",
    "\n",
    "print(\"Written processed data to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa33354b-9672-48dd-9bf6-6f35aece9fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in df_gold: 128025\n",
      "+----------+-------------+------------+------------+--------+----------------+-----------------+----------+----+----+----+--------------+---------------+-----------+-----+----------------+-----+-------+------------+------------+-----+----+----------+-------------+-----+-------------+---+----------------+-------------------+----------------+-----------+---------------+--------------------+---------------+--------------------+--------+----------+\n",
      "|hemisphere|          itz|country_name|country_code|iso_week|spec_received_nb|spec_processed_nb|ah1n1_2009| ah1| ah3| ah5|a_not_subtyped|a_other_subtype|b_vic_nodel|b_yam|b_not_determined|inf_b|inf_all|inf_negative|ili_activity|adeno|boca|metapneumo|parainfluenza|rhino|rsv_processed|rsv|other_resp_virus|psource_subtype_inf|psource_ppos_inf|psource_rsv|total_specimens|     positivity_rate|inf_all_4wk_sum|     pos_rate_4wk_ma|iso_year|who_region|\n",
      "+----------+-------------+------------+------------+--------+----------------+-----------------+----------+----+----+----+--------------+---------------+-----------+-----+----------------+-----+-------+------------+------------+-----+----+----------+-------------+-----+-------------+---+----------------+-------------------+----------------+-----------+---------------+--------------------+---------------+--------------------+--------+----------+\n",
      "|        SH|FLU_TEMP_SAMR|   Argentina|         ARG|       1|            NULL|             1651|         1|NULL|NULL|NULL|             2|           NULL|       NULL| NULL|               1|    1|      4|        1647|        NULL|   14|NULL|         6|           22| NULL|         1651|  9|            NULL|                  2|               2|          2|           1651|0.002422774076317...|             30|0.006891690721483...|    2024|       AMR|\n",
      "|        SH|FLU_TEMP_SAMR|   Argentina|         ARG|       2|            NULL|             2015|         1|NULL|   2|NULL|             8|           NULL|       NULL| NULL|               1|    1|     12|        2003|        NULL|   30|NULL|        11|           18| NULL|         2015| 11|            NULL|                  2|               2|          2|           2015|0.005955334987593052|             30|0.006401632647801516|    2024|       AMR|\n",
      "|        SH|FLU_TEMP_SAMR|   Argentina|         ARG|       3|            NULL|             2321|         3|NULL|   6|NULL|            12|           NULL|       NULL| NULL|               4|    4|     25|        2296|        NULL|   26|NULL|         3|           22| NULL|         2321|  4|            NULL|                  2|               2|          2|           2321|0.010771219302024989|             45|0.006535583839735604|    2024|       AMR|\n",
      "|        SH|FLU_TEMP_SAMR|   Argentina|         ARG|       4|            NULL|             2476|         1|NULL|  14|NULL|            12|           NULL|       NULL| NULL|               1|    1|     28|        2448|        NULL|   27|NULL|         8|           25| NULL|         2476|  1|            NULL|                  2|               2|          2|           2476|0.011308562197092083|             69|0.007614472640756877|    2024|       AMR|\n",
      "|        SH|FLU_TEMP_SAMR|   Argentina|         ARG|       5|            NULL|             2489|         2|NULL|   7|NULL|             9|           NULL|       NULL| NULL|               2|    2|     20|        2469|        NULL|   22|NULL|         3|           15| NULL|         2489|  4|            NULL|                  2|               2|          2|           2489|0.008035355564483728|             85|0.009017618012798463|    2024|       AMR|\n",
      "+----------+-------------+------------+------------+--------+----------------+-----------------+----------+----+----+----+--------------+---------------+-----------+-----+----------------+-----+-------+------------+------------+-----+----+----------+-------------+-----+-------------+---+----------------+-------------------+----------------+-----------+---------------+--------------------+---------------+--------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_gold = spark.read.parquet(\"data/processed/flunet_features\")\n",
    "\n",
    "print(\"Rows in df_gold:\", df_gold.count())\n",
    "df_gold.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b626c48d-d5bb-4e82-96bf-987458a51911",
   "metadata": {},
   "source": [
    "### Now the pipeline will enrich the FluNet data with the external population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abd29bb5-1146-40f9-94ae-5e3e1de9ef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country Name: string (nullable = true)\n",
      " |-- Country Code: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Value: double (nullable = true)\n",
      "\n",
      "+------------+------------+----+-------+\n",
      "|Country Name|Country Code|Year|  Value|\n",
      "+------------+------------+----+-------+\n",
      "|       Aruba|         ABW|1960|54922.0|\n",
      "|       Aruba|         ABW|1961|55578.0|\n",
      "|       Aruba|         ABW|1962|56320.0|\n",
      "|       Aruba|         ABW|1963|57002.0|\n",
      "|       Aruba|         ABW|1964|57619.0|\n",
      "+------------+------------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the population CSV\n",
    "df_pop_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"data/external/population.csv\")\n",
    ")\n",
    "\n",
    "df_pop_raw.printSchema()\n",
    "df_pop_raw.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613990a4-f21c-460e-be9d-9f045dbf347d",
   "metadata": {},
   "source": [
    "### Clean the population data by giving nice names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e940ce2f-82b4-43a9-90ee-6a1ae6186ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      "\n",
      "+------------+----+----------+\n",
      "|country_code|year|population|\n",
      "+------------+----+----------+\n",
      "|         ABW|1960|   54922.0|\n",
      "|         ABW|1961|   55578.0|\n",
      "|         ABW|1962|   56320.0|\n",
      "|         ABW|1963|   57002.0|\n",
      "|         ABW|1964|   57619.0|\n",
      "+------------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleaned population table\n",
    "df_pop = (\n",
    "    df_pop_raw\n",
    "    .select(\n",
    "        F.col(\"Country Code\").alias(\"country_code\"),\n",
    "        F.col(\"Year\").cast(\"int\").alias(\"year\"),\n",
    "        F.col(\"Value\").cast(\"double\").alias(\"population\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_pop.printSchema()\n",
    "df_pop.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd2c989-a39f-4496-9c67-7d8cacc91ac6",
   "metadata": {},
   "source": [
    "## Gold Layer - Enriched Analytical Dataset\n",
    "### We can  now join the population data and the FluNet data.\n",
    "### We want to join on: country_code (3-letter ISO3) & iso_year (from FluNet) = year (from population)\n",
    "### We can do a left join so we can keep all FluNet rows, even if some don't have population data\n",
    "### We'll then add a column for the number of cases per 100k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93add8ef-752d-4c43-9453-658126adee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------+--------+------------+-------+--------------------+--------------------+\n",
      "|who_region|country_name|country_code|iso_year|iso_week|  population|inf_all|    inf_all_per_100k|     positivity_rate|\n",
      "+----------+------------+------------+--------+--------+------------+-------+--------------------+--------------------+\n",
      "|       AFR|      Guinea|         GIN|    2020|      48| 1.3371183E7|      1|0.007478769829116841|0.047619047619047616|\n",
      "|       AFR|        Mali|         MLI|    2020|      34| 2.1713836E7|      2|0.009210717074587834| 0.11764705882352941|\n",
      "|       AFR|       Niger|         NER|    2011|       6| 1.7176283E7|      7|  0.0407538697400363|  0.2916666666666667|\n",
      "|       AFR|      Zambia|         ZMB|    2021|      28| 1.9603607E7|      5|0.025505510286958924|            0.078125|\n",
      "|       AFR|South Africa|         ZAF|    2016|      20| 5.7259551E7|     29| 0.05064657248185547| 0.12236286919831224|\n",
      "|       AFR|Burkina Faso|         BFA|    2022|      34| 2.2509038E7|      2|0.008885319754669213|  0.6666666666666666|\n",
      "|       AMR|      Brazil|         BRA|    1997|      11|1.66661659E8|      1|6.000180281416736E-4|                NULL|\n",
      "|       AMR|       Chile|         CHL|    2016|      10| 1.8267221E7|      7| 0.03832000499692865|0.018421052631578946|\n",
      "|       AMR|   Argentina|         ARG|    2003|      12| 3.8424282E7|      1|0.002602520978791...|                 1.0|\n",
      "|       AMR|      Panama|         PAN|    2017|      39|   4098707.0|      7| 0.17078556725328256| 0.06930693069306931|\n",
      "+----------+------------+------------+--------+--------+------------+-------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join FluNet features with population data\n",
    "df_enriched = (\n",
    "    df_features\n",
    "    .join(\n",
    "        df_pop,\n",
    "        (df_features.country_code == df_pop.country_code) &\n",
    "        (df_features.iso_year == df_pop.year),\n",
    "        how=\"left\"\n",
    "    )\n",
    "    # Drop duplicate columns from population side \n",
    "    .drop(df_pop.country_code)\n",
    "    .drop(\"year\")\n",
    ")\n",
    "\n",
    "# Add per-100k influenza positive rate\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"inf_all_per_100k\",\n",
    "    F.when(\n",
    "        (F.col(\"inf_all\").isNotNull()) &\n",
    "        (F.col(\"population\").isNotNull()) &\n",
    "        (F.col(\"population\") > 0),\n",
    "        (F.col(\"inf_all\") / F.col(\"population\")) * 100000.0\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Inspect some joined rows where we have population and inf_all > 0\n",
    "df_enriched.select(\n",
    "    \"who_region\",\n",
    "    \"country_name\",\n",
    "    \"country_code\",\n",
    "    \"iso_year\",\n",
    "    \"iso_week\",\n",
    "    \"population\",\n",
    "    \"inf_all\",\n",
    "    \"inf_all_per_100k\",\n",
    "    \"positivity_rate\"\n",
    ").where(\n",
    "    (F.col(\"population\").isNotNull()) & (F.col(\"inf_all\") > 0)\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4a7c9-7633-4ed8-8740-5daec398e915",
   "metadata": {},
   "source": [
    "### Next the pipeline restricts the dataset so only rows where population is known are included. It then reports how many rows are left. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e53e6d2-b556-47b0-bad5-585bac2945c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with known population: 108185 out of 128025\n",
      "+----------+------------+--------+--------+------------+--------------------+\n",
      "|who_region|country_name|iso_year|iso_week|  population|    inf_all_per_100k|\n",
      "+----------+------------+--------+--------+------------+--------------------+\n",
      "|       AFR|Sierra Leone|    2018|      23|   7554563.0|                NULL|\n",
      "|       AFR|      Guinea|    2020|      48| 1.3371183E7|0.007478769829116841|\n",
      "|       AFR|        Mali|    2020|      34| 2.1713836E7|0.009210717074587834|\n",
      "|       AFR|       Niger|    2011|       6| 1.7176283E7|  0.0407538697400363|\n",
      "|       AFR|      Zambia|    2021|      28| 1.9603607E7|0.025505510286958924|\n",
      "|       AFR|South Africa|    2016|      20| 5.7259551E7| 0.05064657248185547|\n",
      "|       AFR|South Africa|    2010|      16| 5.2344051E7|                NULL|\n",
      "|       AFR|Burkina Faso|    2022|      34| 2.2509038E7|0.008885319754669213|\n",
      "|       AMR|        Peru|    2021|      11| 3.3155882E7|                NULL|\n",
      "|       AMR|      Brazil|    1997|      11|1.66661659E8|6.000180281416736E-4|\n",
      "+----------+------------+--------+--------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handling missing population and extra rate-based features\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "total_rows_enriched = df_enriched.count()\n",
    "\n",
    "# Keep only rows where we know the population for rate-based analyses\n",
    "df_enriched = df_enriched.filter(F.col(\"population\").isNotNull())\n",
    "\n",
    "print(\"Rows with known population:\", df_enriched.count(), \"out of\", total_rows_enriched)\n",
    "\n",
    "\n",
    "df_enriched.select(\n",
    "    \"who_region\", \"country_name\", \"iso_year\", \"iso_week\",\n",
    "    \"population\", \"inf_all_per_100k\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85814144-4396-43a8-9142-707286567997",
   "metadata": {},
   "source": [
    "### Now we can save this enriched “Gold” dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6544b26-447b-44eb-a492-17aeff8218e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written enriched data to: data/processed/flunet_enriched\n"
     ]
    }
   ],
   "source": [
    "# Final \"gold\" dataset path\n",
    "gold_path = \"data/processed/flunet_enriched\"\n",
    "\n",
    "(\n",
    "    df_enriched\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"iso_year\", \"who_region\")\n",
    "    .parquet(gold_path)\n",
    ")\n",
    "\n",
    "print(\"Written enriched data to:\", gold_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18683f27-5e24-4f9b-aa0c-d369eeaca31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in df_gold_enriched: 108185\n",
      "+----------+------------+------------+--------+--------+-----------+-------+------------------+-----------------+------------------+-----------------+\n",
      "|who_region|country_name|country_code|iso_year|iso_week| population|inf_all|  inf_all_per_100k|  positivity_rate|   pos_rate_4wk_ma|spec_processed_nb|\n",
      "+----------+------------+------------+--------+--------+-----------+-------+------------------+-----------------+------------------+-----------------+\n",
      "|       EUR|     Belgium|         BEL|    2023|       1|1.1787423E7|   1745|14.803914307648075|         54.53125| 55.62374853529087|               32|\n",
      "|       EUR|     Belgium|         BEL|    2023|       2|1.1787423E7|    989| 8.390298710752978|            39.56|58.039389560931895|               25|\n",
      "|       EUR|     Belgium|         BEL|    2023|       3|1.1787423E7|    718|  6.09123809334746|37.78947368421053| 51.13953575976231|               19|\n",
      "|       EUR|     Belgium|         BEL|    2023|       4|1.1787423E7|    938| 7.957634166517991|42.63636363636363| 43.62927183014354|               22|\n",
      "|       EUR|     Belgium|         BEL|    2023|       5|1.1787423E7|   1227|10.409399917182915|58.42857142857143|  44.6036021872864|               21|\n",
      "+----------+------------+------------+--------+--------+-----------+-------+------------------+-----------------+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_gold_enriched = spark.read.parquet(\"data/processed/flunet_enriched\")\n",
    "\n",
    "print(\"Rows in df_gold_enriched:\", df_gold_enriched.count())\n",
    "\n",
    "df_gold_enriched.select(\n",
    "    \"who_region\",\n",
    "    \"country_name\",\n",
    "    \"country_code\",\n",
    "    \"iso_year\",\n",
    "    \"iso_week\",\n",
    "    \"population\",\n",
    "    \"inf_all\",\n",
    "    \"inf_all_per_100k\",\n",
    "    \"positivity_rate\",\n",
    "    \"pos_rate_4wk_ma\",\n",
    "    \"spec_processed_nb\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d25a3e-2d0f-4f1f-ab8c-c504e2aa9602",
   "metadata": {},
   "source": [
    "### Finally, we create high level overview table from our final \"gold\" dataset.\n",
    "\n",
    "### To do this, each WHO region and year is taken, and the total number of positive flu specimens is calculated, the total number of specimens tested is calculated, the average positivity rate, and the average positivity rate per 100,000 incidence is calculated.\n",
    "\n",
    "### This gives us a nice and compact view which is perfect for dashboards or plots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0164fa1d-39ba-48fe-a931-d58cf1bf387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------------------+----------------------+-------------------+--------------------+\n",
      "|who_region|iso_year|total_influenza_positives|total_specimens_tested|avg_positivity_rate|avg_inf_all_per_100k|\n",
      "+----------+--------+-------------------------+----------------------+-------------------+--------------------+\n",
      "|       AFR|    1996|                        9|                   186| 0.1741593567251462|0.024815225828481127|\n",
      "|       AFR|    1997|                      186|                   980| 0.4706595971722022|0.021692869884744732|\n",
      "|       AFR|    1998|                      390|                   494| 0.3161726508785332| 0.07799111605760091|\n",
      "|       AFR|    1999|                      551|                  3094|0.26828093738554776| 0.09683751029029794|\n",
      "|       AFR|    2000|                      154|                  1488|0.17248935974509344| 0.05553801358099915|\n",
      "|       AFR|    2001|                      157|                  1366|0.21671782537183099| 0.06094444512797187|\n",
      "|       AFR|    2002|                      320|                  4808|  0.244729675577116|0.024393826266620334|\n",
      "|       AFR|    2003|                      241|                  1403| 0.3052019895536559|0.027514903307449528|\n",
      "|       AFR|    2004|                      177|                  1282| 0.3584115615542102|0.021181592911605644|\n",
      "|       AFR|    2005|                      762|                  2539| 0.3552851366675831| 0.03562157734996823|\n",
      "|       AFR|    2006|                      830|                  2885| 0.3383988724988979|0.030668677470566966|\n",
      "|       AFR|    2007|                     1777|                 10659| 0.2687014160886783| 0.03451784472726313|\n",
      "|       AFR|    2008|                     2094|                 15035|0.21832853948037062| 0.03673363293229837|\n",
      "|       AFR|    2009|                    10232|                 40535|0.32148159490760736| 0.12017951547686737|\n",
      "|       AFR|    2010|                     7390|                 47807|0.22155695939341152| 0.07073190292338867|\n",
      "|       AFR|    2011|                     7789|                 43841|0.22035294628615462| 0.05259272121398159|\n",
      "|       AFR|    2012|                     6890|                 50658|0.18966243235793934|0.051299399624858064|\n",
      "|       AFR|    2013|                     6256|                 44866|0.20117825486833324| 0.04728867528354656|\n",
      "|       AFR|    2014|                     4530|                 32422| 0.2004113706347951|0.037641535261669516|\n",
      "|       AFR|    2015|                     4696|                 29538|0.20110768994304323|0.030208008751793327|\n",
      "+----------+--------+-------------------------+----------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Start from your saved gold dataset\n",
    "df_gold_enriched = spark.read.parquet(\"data/processed/flunet_enriched\")\n",
    "\n",
    "# Summary by WHO region and year\n",
    "df_region_year = (\n",
    "    df_gold_enriched\n",
    "    .groupBy(\"who_region\", \"iso_year\")\n",
    "    .agg(\n",
    "        F.sum(\"inf_all\").alias(\"total_influenza_positives\"),\n",
    "        F.sum(\"total_specimens\").alias(\"total_specimens_tested\"),\n",
    "        F.avg(\"positivity_rate\").alias(\"avg_positivity_rate\"),\n",
    "        F.avg(\"inf_all_per_100k\").alias(\"avg_inf_all_per_100k\")\n",
    "    )\n",
    "    .orderBy(\"who_region\", \"iso_year\")\n",
    ")\n",
    "\n",
    "df_region_year.show(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb7919d-afcf-4a59-84ec-4d40005db2f1",
   "metadata": {},
   "source": [
    "### Additionally, we created a CSV output for downstream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b77eef47-0de7-443e-88b6-9fa1db16c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote CSV export of gold dataset to: data/processed/flunet_enriched_csv\n",
      "Note: Spark writes a folder with one part-*.csv file inside it.\n"
     ]
    }
   ],
   "source": [
    "csv_output_path = \"data/processed/flunet_enriched_csv\"\n",
    "\n",
    "(\n",
    "    df_gold_enriched\n",
    "    .coalesce(1)  # put all data into 1 partition so we get a single CSV part file\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", True)\n",
    "    .csv(csv_output_path)\n",
    ")\n",
    "\n",
    "print(f\"Wrote CSV export of gold dataset to: {csv_output_path}\")\n",
    "print(\"Note: Spark writes a folder with one part-*.csv file inside it.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
